Week 7 -- THE FINAL WEEK -- Watching even more Andrej Karpathy, Working with our pre-existing LLM to accept more datasets at once for a more knowledgeable final product

Mon - Thurs - HuggingFace Transformers documentation / HuggingFace Datasets Documentation / YES, EVEN MORE Andrej Karpathy Tutorials & Documentation / LLM Coding Tutorials / Stephen Wolfram ChatGPT book

Fri - Sun - Working with Week 5 and 6's LLM to optimize the most knowledge and capabilities with a reasonable amount of time for processing (final product took 3+ hours to train)

** Issues I ran into while building this**
-- Problem-solving and patience were tested the most for this model. From running into compatibility issues, to having datasets balloon to over 100+gb within the training process, this was a tough one. One dataset is nothing compared to this.

--- **The Coded Deliverable(s):
--- "Curiosity-14 LLM + Checkpoint-1500 Configs (Most Files)" 

** How Curiosity-14 (LLM) works** 
-- Imports GPT-2 and GPT-2 Tokenizer (for local training and fine-tuning, no need for APIs on OpenAI playground)

-- Imports HuggingFace transformer library packages (I.e DataCollator, Trainer, etc) -- "import torch #pytorch for training purposes
from accelerate import Accelerator # Need this to address numpy issues
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling #OpenAI and HuggingFace packages for training, training configs, and data batch preparation
from datasets import load_dataset, concatenate_datasets # hugging face datasets"

-- Padding Token so that the varying lengths of tokens can be processed # Set the padding token to the EOS token -- This keeps tokenization for sequences consistent, keep attention masking simplified

-- Loads more than one dataset so that we have multiple datasets to work with, which should help our model become more capable. (utilizes split function for using less of the training set for faster processing speeds, also runs remote code for loading dataset)
-- # Load datasets from Huggingface.
comb_ds = load_dataset("yoonholee/combined-preference-dataset", split='train[:1%]', trust_remote_code=True)
pref_ds = load_dataset("OpenRLHF/preference_dataset_mixture2_and_safe_pku", split='train[:1%]', trust_remote_code=True)
com_ds = load_dataset("community-datasets/generics_kb", "generics_kb_simplewiki", split='train[:1%]', trust_remote_code=True)

-- # Combine dataset(s), make sure your datasets are compatible with each other
combined_dataset = concatenate_datasets([comb_ds, pref_ds, com_ds])

-- Preprocessing functions for truncating, tokenizing the data, applying that format to the entire data, and converting tokenized dataset into tensors
-- # Preprocess function for the combined dataset
def preprocess_function(examples): #Looks for examples as input, used as dictionary
    # Text fields can be adjusted based on data columns for dataset(s)
    text_fields = ['text', 'chosen', 'rejected', 'content', 'sentence', 'concept_name'] #Adjusted for dataset(s) columns, looks for keywords in examples dictionay
    for field in text_fields: # Goes through list of text fields (loops), if field exists it assigns value to texts and leaves loop
        if field in examples:
            texts = examples[field]
            break
    else:
        raise ValueError(f"No available text fields were found: {examples.keys()}") # If no assigned values are found, the program breaks
    # Elements MUST be Strings (or it will break)
    texts = [str(text) if text is not None else "" for text in texts]
    return tokenizer(texts, truncation=True, padding='max_length', max_length=256) # Adjust if needed -- uniformity in sequence tokenization, longer sequences are truncated

-- Debugging statements for checking compatibility (Not all datasets are set up the same, so this is a good way to check the columns, the keywords for the text_fields, and the sample data of a dataset as it is loaded in
-- # Print dataset (column) information (also good for debugging when your combined dataset(s) don't work together
print("Dataset columns:", combined_dataset.column_names)
print("Sample data from datasets:")
print(combined_dataset[:5])

-- # Tokenize the combined dataset(s)
tokenized_datasets = combined_dataset.map(preprocess_function, batched=True,
remove_columns=combined_dataset.column_names)
tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask'])


# Finding (len) size of dataset(s) for future partitioning (breaking into smaller sets)
dataset_size = len(tokenized_datasets)

# Define the size of the subsets, for training sets and eval sets, good for setting sizes later
train_size = min(1000, dataset_size)
eval_size = min(200, dataset_size)


-- Sets a smaller sample size for the dataset and shuffles evaluation data
# Partition dataset into smaller sets for faster processing sepeeds and time
small_train_dataset = tokenized_datasets.shuffle(seed=42).select(range(train_size))
small_eval_dataset = tokenized_datasets.shuffle(seed=42).select(range(eval_size))

-- Training Arguments sets the filepath, sets evaluations at the end of each epoch, sets learning rate at 2e-5, sets batch size for each training and evaluation per device,
  number of epochs to train, sets weight_decay so the weights don't get too large, saves checkpoint number (2) for trainer
-- # Define training args
training_args = TrainingArguments(
    output_dir='./results',
    eval_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=2,  # Smaller batch size for faster processing speeds/time
    per_device_eval_batch_size=2,  # Smaller batch size for faster processing speeds/time
    num_train_epochs=3,  # Increase number of epochs (cycles of running through)
    weight_decay=0.01,
    save_total_limit=2,  # Number of checkpoints that will be saved
)

-- Data Collator handles labels in batches in the dataset and handles the data for training, sets MLM to false (Everyone else does this, who doesn't use BERT, we're using GPT-2)
# Data collator function (batching samples from training set), disabling Masked Language Modelling (no BERT)
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False, # No BERT
)

-- Trainer() is self-explanatory, parameters are set for datasets, the model, tokenizer (etc)
-- # Trainer is set up to work with smaller datasets
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset, # this is all self explanatory
    eval_dataset=small_eval_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

# Train model function
trainer.train()
-- Trainer() function 

-- Interactive Prompt for User: 
-- We still need GPT-2/GPT-2 Tokenizer to build on what we did earlier for text generation
-- Loads checkpoint/saved model from filepath, loads tokenizer and model
-- Pipeline sets up connection between text-generation and the model + tokenizer
-- While loop does as written, the interactive aspect of Curiosity-14
--from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline 

# Loading GPT-2 + GPT-2 Tokenizer + Checkpoint filePATH
model_path = '/Users/kharazmimac/PycharmProjects/LLMScriptTest14/results/checkpoint-1500'
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

# Set up pipeline for text generation (relating to user prompt)
text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)

# Interactive Prompt for user, generate text based on user's entered prompt
while True:
    text = input("Enter a prompt: ")
    if text.lower() == 'exit':
        break
    result = text_generator(text, max_length=256, num_return_sequences=1)
    print(result[0]['generated_text'])

**Pros! (This is the final model for the EEP!!)
-- Answers are more "human-like" compared to previous models.
-- Answers are more capable compared to previous models.
-- Understands some problem-solving, i.e simple coding questions (see demo images)
-- Answers are mostly cleaner and less disjointed than previous models
-- Took 3+ hours to train, but we have a more capable model than before that is lightweight in terms of storage taken up
-- I'm incredibly proud of this model and the work it took to get here
-- Lots of variability in answers given (good or bad)

**Interesting note**
Using capitals might grant you a different answer even if the prompt is the exact same but lowercase
Rewriting the question again, however different or not different, will yield different results, not all the same. Like ChatGPT or any other Chatbot LLM.

**Shortcomings**

-- This model took 3+ hours to train, wish I knew how to make it faster
-- Limited in scope compared to LLMs made by big corporations
-- Full potential hasn't been reached yet
-- Did not have the time to code in a 'regenerate' answer prompt that could give the user another opportunity to get a better answer... Maybe in the future.
-- Some answers still have a weird relation to the prompt, not always a perfect connection for prompt --> answer
